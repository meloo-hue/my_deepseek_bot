import feedparser
import aiohttp
import asyncio
import logging
from typing import List, Dict, Optional
from datetime import datetime
import html
import re

logger = logging.getLogger(__name__)

class RSSNewsEngine:
    """–ü–æ–∏—Å–∫–æ–≤—ã–π –¥–≤–∏–∂–æ–∫ –Ω–∞ –±–∞–∑–µ RSS-–ª–µ–Ω—Ç —Ä–æ—Å—Å–∏–π—Å–∫–∏—Ö –°–ú–ò"""
    
    def __init__(self):
        self.sources = {
            # –ì–ª–∞–≤–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–Ω—ã–µ –∞–≥–µ–Ω—Ç—Å—Ç–≤–∞
            "ria": {
                "url": "https://ria.ru/export/rss2/index.xml",
                "name": "–†–ò–ê –ù–æ–≤–æ—Å—Ç–∏"
            },
            "tass": {
                "url": "http://tass.ru/rss/v2.xml",
                "name": "–¢–ê–°–°"
            },
            "interfax": {
                "url": "https://www.interfax.ru/rss.asp",
                "name": "–ò–Ω—Ç–µ—Ä—Ñ–∞–∫—Å"
            },
            "rbc": {
                "url": "https://rssexport.rbc.ru/rbc/news/20/full.rss",
                "name": "–†–ë–ö"
            },
            
            # –ì–∞–∑–µ—Ç—ã
            "rg": {
                "url": "https://rg.ru/xml/index.xml",
                "name": "–†–æ—Å—Å–∏–π—Å–∫–∞—è –ì–∞–∑–µ—Ç–∞"
            },
            "kp": {
                "url": "https://www.kp.ru/rss/alls.xml",
                "name": "–ö–æ–º—Å–æ–º–æ–ª—å—Å–∫–∞—è –ü—Ä–∞–≤–¥–∞"
            },
            "mk": {
                "url": "https://www.mk.ru/rss/news/index.xml",
                "name": "–ú–æ—Å–∫–æ–≤—Å–∫–∏–π –ö–æ–º—Å–æ–º–æ–ª–µ—Ü"
            },
            "iz": {
                "url": "https://iz.ru/xml/rss/all.xml",
                "name": "–ò–∑–≤–µ—Å—Ç–∏—è"
            },
            "aif": {
                "url": "https://aif.ru/rss/all.php",
                "name": "–ê—Ä–≥—É–º–µ–Ω—Ç—ã –∏ –§–∞–∫—Ç—ã"
            },
            "kommersant": {
                "url": "https://www.kommersant.ru/RSS/news.xml",
                "name": "–ö–æ–º–º–µ—Ä—Å–∞–Ω—Ç—ä"
            },
            "vedomosti": {
                "url": "https://vedomosti.ru/rss/news",
                "name": "–í–µ–¥–æ–º–æ—Å—Ç–∏"
            },
            
            # –û–Ω–ª–∞–π–Ω-–∏–∑–¥–∞–Ω–∏—è
            "lenta": {
                "url": "https://lenta.ru/rss/news",
                "name": "Lenta.ru"
            },
            "gazeta": {
                "url": "https://www.gazeta.ru/export/rss/full.xml",
                "name": "Gazeta.ru"
            },
            "life": {
                "url": "https://life.ru/life.rss",
                "name": "Life.ru"
            },
            
            # –†–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–µ
            "fontanka": {
                "url": "https://www.fontanka.ru/fontanka.rss",
                "name": "–§–æ–Ω—Ç–∞–Ω–∫–∞.—Ä—É"
            },
            "dp": {
                "url": "https://www.dp.ru/export/export-dp-rss.xml",
                "name": "–î–µ–ª–æ–≤–æ–π –ü–µ—Ç–µ—Ä–±—É—Ä–≥"
            }
        }
        
        self.queries_today = 0
        self.max_daily = 1000  # –£—Å–ª–æ–≤–Ω—ã–π –ª–∏–º–∏—Ç, –Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ –±–µ–∑–ª–∏–º–∏—Ç–Ω–æ
        self.last_reset = datetime.now().date()
    
    def _check_limits(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–Ω–µ–≤–Ω–æ–≥–æ –ª–∏–º–∏—Ç–∞ (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)"""
        today = datetime.now().date()
        if today != self.last_reset:
            self.queries_today = 0
            self.last_reset = today
        
        if self.queries_today >= self.max_daily:
            logger.warning(f"‚ö†Ô∏è –î–Ω–µ–≤–Ω–æ–π –ª–∏–º–∏—Ç RSS ({self.max_daily})")
            return False
        return True
    
    def _parse_date(self, published: str) -> str:
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –¥–∞—Ç—É –∏–∑ RSS –≤ —á–∏—Ç–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç"""
        if not published:
            return ""
        
        try:
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –¥–∞—Ç
            if 'T' in published:  # ISO —Ñ–æ—Ä–º–∞—Ç
                pub_date = datetime.fromisoformat(published.replace('Z', '+00:00'))
                return pub_date.strftime("%d.%m.%Y %H:%M")
            elif 'GMT' in published:
                pub_date = datetime.strptime(published, "%a, %d %b %Y %H:%M:%S GMT")
                return pub_date.strftime("%d.%m.%Y %H:%M")
            else:
                # –ü—Ä–æ—Å—Ç–æ –æ–±—Ä–µ–∑–∞–µ–º –¥–æ –ø–µ—Ä–≤—ã—Ö 16 —Å–∏–º–≤–æ–ª–æ–≤
                return published[:16]
        except:
            return published[:10]
    
    async def get_latest_news(self, source: str = "all", limit: int = 5) -> List[Dict]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ —É–∫–∞–∑–∞–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        
        Args:
            source: "all" –∏–ª–∏ –∫–ª—é—á –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "ria")
            limit: –°–∫–æ–ª—å–∫–æ –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ–ª—É—á–∏—Ç—å
        """
        if not self._check_limits():
            return []
        
        results = []
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –∫–∞–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å
        if source == "all":
            sources_to_check = list(self.sources.items())
        else:
            sources_to_check = [(source, self.sources.get(source, {}))]
        
        for src_key, src_info in sources_to_check:
            if not src_info:
                continue
                
            try:
                logger.info(f"üì° –ß–∏—Ç–∞—é RSS: {src_info['name']}")
                
                # –ü–∞—Ä—Å–∏–º RSS (feedparser —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π, –∑–∞–ø—É—Å–∫–∞–µ–º –≤ –ø–æ—Ç–æ–∫–µ)
                loop = asyncio.get_event_loop()
                feed = await loop.run_in_executor(
                    None, 
                    lambda: feedparser.parse(src_info['url'])
                )
                
                if not feed.entries:
                    logger.warning(f"‚ö†Ô∏è –ü—É—Å—Ç–æ–π RSS: {src_info['name']}")
                    continue
                
                for entry in feed.entries[:3]:  # –ë–µ—Ä–µ–º –ø–æ 3 –∏–∑ –∫–∞–∂–¥–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
                    # –û—á–∏—â–∞–µ–º HTML –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞
                    title = html.unescape(entry.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è'))
                    title = re.sub('<[^<]+?>', '', title)
                    
                    # –°—Å—ã–ª–∫–∞ –Ω–∞ –Ω–æ–≤–æ—Å—Ç—å
                    link = entry.get('link', '')
                    
                    # –ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
                    summary = html.unescape(entry.get('summary', ''))
                    summary = re.sub('<[^<]+?>', '', summary)[:200]
                    
                    # –î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
                    published = entry.get('published', '')
                    date = self._parse_date(published)
                    
                    results.append({
                        "title": title,
                        "content": summary,
                        "url": link,
                        "date": date,
                        "source": src_info['name'],
                        "source_key": src_key,
                        "is_russian": True
                    })
                    
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ RSS {src_info.get('name', src_key)}: {e}")
                continue
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ (—Å–≤–µ–∂–∏–µ —Å–≤–µ—Ä—Ö—É)
        results.sort(key=lambda x: x['date'], reverse=True)
        
        self.queries_today += 1
        logger.info(f"‚úÖ RSS: —Å–æ–±—Ä–∞–Ω–æ {len(results)} –Ω–æ–≤–æ—Å—Ç–µ–π")
        
        return results[:limit]
    
    async def search_news(self, query: str, limit: int = 5) -> List[Dict]:
        """
        –ò—â–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            limit: –°–∫–æ–ª—å–∫–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤–µ—Ä–Ω—É—Ç—å
        """
        # –ü–æ–ª—É—á–∞–µ–º —Å–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏
        all_news = await self.get_latest_news(limit=50)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –∑–∞–ø—Ä–æ—Å—É
        query_lower = query.lower()
        filtered = []
        
        for news in all_news:
            title_lower = news['title'].lower()
            content_lower = news['content'].lower()
            
            if (query_lower in title_lower or 
                query_lower in content_lower):
                filtered.append(news)
            
            if len(filtered) >= limit:
                break
        
        return filtered[:limit]
    
    def format_news_results(self, results: List[Dict], query: str = "") -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ —á–∞—Ç"""
        if not results:
            return "üì∞ –ù–æ–≤–æ—Å—Ç–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ."
        
        if query:
            message = f"üì∞ **–ù–æ–≤–æ—Å—Ç–∏ –ø–æ –∑–∞–ø—Ä–æ—Å—É:**\n"
            message += f"_{query}_\n\n"
        else:
            message = f"üì∞ **–°–≤–µ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏:**\n\n"
        
        for i, item in enumerate(results, 1):
            title = item.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')
            source = item.get('source', '–ù–æ–≤–æ—Å—Ç–∏')
            date = item.get('date', '')
            url = item.get('url', '')
            
            message += f"**{i}. üá∑üá∫ {title}**\n"
            message += f"üì∞ {source}"
            if date:
                message += f" üìÖ {date}"
            message += "\n"
            
            if item.get('content'):
                message += f"_{item['content']}_\n"
            
            if url:
                message += f"üîó [–ß–∏—Ç–∞—Ç—å]({url})\n"
            message += "\n"
        
        return message.strip()
    
    def get_sources_list(self) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        sources = []
        for key, info in self.sources.items():
            sources.append(f"‚Ä¢ {info['name']} (/{key})")
        
        return "üì∞ **–î–æ—Å—Ç—É–ø–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏:**\n" + "\n".join(sources)


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
rss_news = RSSNewsEngine()